%%% packages %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
\documentclass[frenchb]{report}
%\usepackage{natbib}
\usepackage[dvipsnames]{xcolor}
\usepackage[french]{babel}
\usepackage{url}
\usepackage[utf8x]{inputenc}
\usepackage{graphicx}
\graphicspath{{images/}}
\usepackage{parskip}
\usepackage{fancyhdr}
\usepackage{fancyvrb}
\usepackage{vmargin}
\usepackage{xcolor}
\usepackage{bbm}
\usepackage{amsmath,amssymb}
\usepackage{amsthm}
\usepackage{dsfont}
\usepackage{stmaryrd}
\usepackage{systeme}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{pifont}
\usepackage{textcomp}
\usepackage{calrsfs}
\usepackage[T1]{fontenc}
\usepackage[toc,page]{appendix}
\usepackage{lipsum}
\usepackage{verbatim}
\usepackage{listings}
\usepackage{adforn}
\usepackage{float}
% Les packages necessaires pour faire le pseudo code
%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{algorithm}
\usepackage{algorithmic}
%%%%%%%%%%%%%%%%%%%%%%%%%
% Je renomme les commandes en français
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\renewcommand{\algorithmicrequire}{\textbf{Entrée(s) :}}
\renewcommand{\algorithmicreturn}{\textbf{retourner}}
\renewcommand{\algorithmicensure}{\textbf{Initialisation ;}}
\renewcommand{\algorithmicwhile}{\textbf{Tant que}}
\renewcommand{\algorithmicdo}{\textbf{Initialisation}}
\renewcommand{\algorithmicendwhile}{\textbf{fin du Tant que ;}}
\renewcommand{\algorithmicend}{\textbf{fin}}
\renewcommand{\algorithmicif}{\textbf{si}}
\renewcommand{\algorithmicendif}{\textbf{fin du si}}
\renewcommand{\algorithmicelse}{\textbf{sinon}}
\renewcommand{\algorithmicelsif}{\textbf{fin du sinon}}
\renewcommand{\algorithmicthen}{\textbf{alors}}
\renewcommand{\algorithmicthen}{\textbf{Étape E}}
\renewcommand{\algorithmicthen}{\textbf{Étape M}}
\renewcommand{\algorithmicfor}{\textbf{pour}}
\renewcommand{\algorithmicforall}{\textbf{pour tout}}
\renewcommand{\algorithmicto}{\textbf{à}}
\renewcommand{\algorithmicendfor}{\textbf{fin du pour}}
\renewcommand{\algorithmicdo}{\textbf{faire}}
\renewcommand{\algorithmicloop}{\textbf{boucler}}
\renewcommand{\algorithmicendloop}{\textbf{fin de la boucle}}
\renewcommand{\algorithmicrepeat}{\textbf{répéter}}
\renewcommand{\algorithmicuntil}{\textbf{jusqu’à}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setlength{\hoffset}{-18pt}        
\setlength{\oddsidemargin}{0pt} % Marge gauche sur pages impaires
\setlength{\evensidemargin}{9pt} % Marge gauche sur pages paires
\setlength{\marginparwidth}{54pt} % Largeur de note dans la marge
\setlength{\textwidth}{481pt} % Largeur de la zone de texte (17cm)
\setlength{\voffset}{-18pt} % Bon pour DOS
\setlength{\marginparsep}{7pt} % Séparation de la marge
\setlength{\topmargin}{0pt} % Pas de marge en haut
\setlength{\headheight}{13pt} % Haut de page
\setlength{\headsep}{10pt} % Entre le haut de page et le texte
\setlength{\footskip}{27pt} % Bas de page + séparation
\setlength{\textheight}{708pt} % Hauteur de la zone de texte (25cm)
\usepackage{hyperref}
\lstset{%
            inputencoding=utf8,
                extendedchars=true,
                literate=%
                {é}{{\'e}}{1}%
                {è}{{\`e}}{1}%
                {à}{{\`a}}{1}%
                {ç}{{\c{c}}}{1}%
                {œ}{{\oe}}{1}%
                {ù}{{\`u}}{1}%
                {É}{{\'E}}{1}%
                {È}{{\`E}}{1}%
                {À}{{\`A}}{1}%
                {Ç}{{\c{C}}}{1}%
                {Œ}{{\OE}}{1}%
                {Ê}{{\^E}}{1}%
                {ê}{{\^e}}{1}%
                {î}{{\^i}}{1}%
                {ô}{{\^o}}{1}%
                {û}{{\^u}}{1}%
                {ë}{{\¨{e}}}1
                {û}{{\^{u}}}1
                {â}{{\^{a}}}1
                {Â}{{\^{A}}}1
                {Î}{{\^{I}}}1
        }
    
    
\lstset{ language=R,
  backgroundcolor=\color{MidnightBlue},   % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}; should come as last argument
   basicstyle=\small\ttfamily\color{white},        % the size of the fonts that are used for the code
  breakatwhitespace=false,         % sets if automatic breaks should only happen at whitespace
  breaklines=true,                 % sets automatic line breaking
  captionpos=b,                    % sets the caption-position to bottom
  commentstyle=\color{SpringGreen},    % comment style
  deletekeywords={data,frame,length,as,character},           % if you want to delete keywords from the given language
  extendedchars=true,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
  frame=single,	                   % adds a frame around the code
  keepspaces=true,                 % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
   keywordstyle=\color{Peach},       % keyword style
  morekeywords={kable,*,...},            % if you want to add more keywords to the set
  numbers=left,                    % where to put the line-numbers; possible values are (none, left, right)
  numbersep=5pt,                   % how far the line-numbers are from the code
  %numberstyle=\tiny\color{gray}, % the style that is used for the line-numbers
  rulecolor=\color{white},         % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  showspaces=false,                % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
  showstringspaces=false,          % underline spaces within strings only
  showtabs=false,                  % show tabs within strings adding particular underscores
  stepnumber=2,                    % the step between two line-numbers. If it's 1, each line will be numbered
      % string literal style
  tabsize=2,	                   % sets default tabsize to 2 spaces
  title=\lstname,                  % show the filename of files included with \lstinputlisting; also try caption instead of title
  mathescape=true,
  escapechar=|
  }
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%        


        
\makeatletter
\let\thetitle\@title
\let\theauthor\@author
\let\thedate\@date
\makeatother

%%% commandes mise en page %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%        
\newcommand{\ld}{\log_{2}}
\newcommand{\R}{\mathbbm{R}}
\newcommand{\N}{\mathbbm{N}}
\newcommand{\1}{\mathbbm{1}}
\newcommand{\E}{\mathbbm{E}}
\newcommand{\V}{\mathbbm{V}}
\newcommand{\prob}{\mathbbm{P}}
\newcommand{\Nc}{\mathcal{N}}
\newcommand{\Cc}{\mathcal{C}}
\newcommand{\K}{\mathcal{K}}
\newcommand{\Xti}{\widetilde{X_i}}
\newcommand{\Xtj}{\widetilde{X_j}}
\newcommand{\Xn}{\overline{X_n}}
\newcommand{\gn}{\hat{g_n}}
\newcommand{\n}{\mathcal{N}}
\newcommand{\lv}{\mathcal{L}}
\newcommand{\thetat}{\tilde{\theta}}

\newcommand{\console}[1]{\colorbox{black}{\begin{minipage}[c]{1\linewidth}\textcolor{white}{\texttt{#1}}\end{minipage}}}

\newtheorem{prop}{Proposition}
\newtheorem{thm}{Théorème}
\newtheorem{cor}{Corollaire}
\newtheorem{lem}{Lemme}
\newtheorem{hyp}{Hypothèse}
\theoremstyle{definition}\newtheorem{defn}{Définition}
\theoremstyle{definition}\newtheorem{exm}{Exemple}
\theoremstyle{definition}\newtheorem{nota}{Notation}
\theoremstyle{definition}\newtheorem{rem}{Remarque}

\renewcommand{\qedsymbol}{\adfhangingflatleafright}


\begin{document}

%%% Page de garde %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{titlepage}
\begin{center}
\includegraphics[scale=0.5]{logo.png}\\[1cm]
{\LARGE Université de Montpellier}\\[1.5cm]
\linespread{1.2}\huge {\bfseries Projet M1 SSD }\\[0.5cm]
\linespread{1.2}\LARGE {\bfseries Un modèle pour les nids de mouettes}\\[1.5cm]
\linespread{1}

{\large Rédigé par\\}
{\Large \textsc{carvaillo} Thomas}\\
{\Large \textsc{côme} Olivier}\\
{\Large \textsc{pralon} Nicolas}\\[1cm]
{\large \emph{Encadrante :} Elodie \textsc{Brunel-Piccinini}}\\[1cm] % if applicable
%\textit{dans le}\\[0.3cm]
%Départment d'Informatique, UFR Sciences et Techniques\\[2cm]
\today
\end{center}
\end{titlepage}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\tableofcontents
\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter*{Introduction}
\addcontentsline{toc}{part}{Introduction}


\pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%% Premier chapitre %%%
\chapter{Modélisation du problème}

% section 1
\section{Modélisation du problème}

Nous allons pour commencer donner une première définition, qui est au coeur du présent projet.

\begin{defn}[Loi de mélange]
On appelle loi de mélange toute loi dont la densité s'écrit sous la forme d'une combinaison convexe de diverses densités. C'est-à-dire que si l'on se donne $J$ variables aléatoires $X_1, \cdots, X_J$ de densité respective $f_1(x), \cdots, f_J(x)$, alors est appellée loi de mélange toute variable aléatoire $X$ dont la densité $f$ s'exprime sous la forme
\begin{center} $f(x) := \displaystyle\sum_{i=1}^J \alpha_i f_i(x)$ , $\alpha_i \in\R$ \end{center}

\end{defn}

Afin de modéliser commodément le problème, nous introduisons les variables aléatoires suivantes :

\begin{itemize}[label=\adfflowerleft]
	\item La variable aléatoire $X$, modélisant la taille des nids
	\item Et $Z$, la variable aléatoire représentant l'espèce de mouette qui a construit le nid
\end{itemize}

Enfin, nous nous placerons sous les hypothèses suivantes:

\begin{hyp}
Nous supposerons que, $\forall j\in \llbracket 1,J \rrbracket$, la taille des nids d'une espèce $j$ ( \underline{i.e.} $X$ conditionnellement à $(Z=j)$ ) suit une loi normale $\n(\mu_j,v_j)$. Nous dénoterons par $\gamma_{\mu_j, v_j}(x)$ cette densité.
\end{hyp}


\begin{hyp}
Soit $\Theta := \{ \theta = (\alpha_j,\mu_j, v_j)_{1 \leq j \leq J} \text{ tels que } \alpha_j > 0 \text{ } \forall j\in \llbracket 1,J\rrbracket \text{ et } \displaystyle\sum_{j=1}^J\alpha_j=1\}$. Soient $X_1, \cdots, X_n$ un échantillon de même loi que $X$. On supposera qu'il existe un $\theta \in \Theta$ tel que les données récoltées, ici les tailles des nids, soient la réalisation du précédent échantillon.
\end{hyp}

\begin{prop}
La variable $Z$ est discrète et à valeur dans un sous-ensemble fini de $\N$, elle suit donc une loi 
\begin{center} $\displaystyle \sum_{j=1}^J \alpha_j\delta_j$ \end{center}
\underline{où} $J$ représente le nombre d'espèce de mouettes considéré et les $\alpha(j)$ sont des réels, positifs stricts, représentant la proportion de nids de l'espèce $j$, tels que $\displaystyle\sum_{i=1}^J \alpha_j = 1$.
\end{prop}

Il s'ensuit la proposition suivante, qui sera la racine du présent projet.
\begin{prop}
La distribution de la taille des nids de mouettes, \underline{i.e.} $X$, admet pour densité ,au point $x$ et par rapport à la mesure de Lebesgue sur $\R$, la fonction $f_ \theta$ définie comme suit
\begin{center} $f_\theta(x) = \displaystyle\sum_{j=1}^J \alpha_j \gamma_{\mu_j, v_j}(x) $ \end{center}
\end{prop}

\begin{proof}
On vérifie que l'on obtient bien une densité de probabilité, la forme de cette dernière étant la conséquence directe de la définition de la variable aléatoire $X$ : \newline
\begin{center} $\displaystyle\int_\R f_\theta(x) dx = \int_\R\sum_{j=1}^J \alpha_j \gamma_{\mu_j, v_j}(x)dx = \sum_{j=1}^J\alpha_j\int_\R \gamma_{\mu_j, v_j}(x)dx = \sum_{j=1}^J\alpha_j = 1$ \end{center}
\end{proof}

Le but de ce projet sera d'étudier des méthodes permettant l'estimation des divers paramètres de cette densité. Nous détonerons par $\theta := (\alpha_j, \mu_j, v_j)_{1\leq j\leq J}$ les vecteurs des ces dits paramètres.

\begin{nota}[Densités]
Le vecteur $\theta$ ayant été dûment introduit, nous noterons
\begin{enumerate}
	\item $\prob_\theta(Z=j) := \alpha_j$ la probabilité que la variable aléatoire $Z$ prenne la valeur $j$
	\item $f_\theta(x | Z=j) := \gamma_{\mu_j, v_j}(x)$  la densité de la loi de $X$ sachant $Z$
\end{enumerate}
qui sont, respectivement, contre la mesure de comptage sur $\N$, et par rapport à la mesure de $\mathcal{L}$ebesgue sur $\R$,
\end{nota}

Introduisons deux dernières densités, qui nous seront fort utile quant à l'expression des Log-vraisemblances conditionnelles :

\begin{prop}[Loi de densité du vecteur (X,Z) et loi de densité de Z sachant X]
La densité du vecteur aléatoire $(X, Z)$ est donnée par :
\begin{align*} 
h_\theta : &~(R\times \left\{1,\cdots,J\right\}) \rightarrow R_+\\
&~(x,z) \mapsto \alpha_j\times\gamma_{\mu_j, v_j}(x) 
\end{align*}
\end{prop}

\begin{proof}
En utilisant la réciproque du théorème de transfert et la densité conditionnelle de $X$ sachant $Z$ et $Z$ sachant $X$, on obtient ainsi :
\begin{align*} 
\E[h(X,Z)] & = \int_{R\times\left\{1,\cdots,J\right\}}h(x,z)\times h_\theta(x,z)~d\lambda(x)\times dN(z)\\
&= \int_{R\times\left\{1,\cdots,J\right\}}h(x,z)\times f_{X|Z}(x) f_Z(z)~d\lambda(x)\times dN(z)\\
&= \int_{R\times\left\{1,\cdots,J\right\}}h(x,z)\times \gamma_{m(z),v(z)}(x)\alpha_z~d\lambda(x)\times dN(z)\\
&= \int_{R\times\left\{1,\cdots,J\right\}}h(x,z)\times f_{Z|X}(z) f_X(x)~d\lambda(x)\times dN(z)\\
\end{align*}
Par identification on en déduit, la densité du couple $(X,Z)$
\begin{align*}
f_{Z|X}(z) f_X(x) &=  f_{X|Z}(x) f_Z(z)\\
&= \gamma_{m(z),v(z)}\alpha_z
\end{align*}
et puis la de densité conditionnelle de $Z$ sachant $X$
\begin{center}
$f_{Z|X}(z) = \frac{\gamma_{m(z),v(z)}\alpha_z}{\sum_{i=1}^J \alpha_i \times \gamma_{m(i), v(i)}(x)}$
\end{center}
\end{proof}

\begin{rem}
Nous pouvons dès à présent noter que pour un échantillon $X_1, \cdots, X_n$ de même loi que $X$, nous avons 
\begin{center}
$\forall i \in \llbracket 1,n \rrbracket$, $h_\theta(X_i,j) = f_\theta(X_i) \times \prob_\theta(Z = j | X = X_i )$
\end{center}
Ceci nous sera utile dans la suite.
\end{rem}

Nous allons dès à présent nous intéresser à l'estimation de ces paramètres.

% Section 2
\section{Un cas élémentaire}

Regardons dans un premier temps un cas simplifié, un cas ne décrivant pas la réalité des observations mais qui a le mérite de constituer une agréable entrée en matière. \newline
Nous supposerons ici qu'ont été relevés simultanément et les mesures des tailles des nids et l'espèce de mouette qui l'a construit. Le modèle ici considéré est donc composé des couples $(X_i, Z_i)$, $i \in \llbracket1,n \rrbracket$. On considérera dès lors la fonction de densité $h_\theta(x,z)$.\newline
L'estimation des divers paramètres est alors élémentaire, en témoigne les propositions suivantes :
\begin{prop}[Fonction de Log-vraisemblance]
La Log-vraisemblance du modèle s'écrit
\begin{center} $\mathcal{L}_\theta(X_1, \cdots, X_n, Z_1, \cdots, Z_n) = \displaystyle \sum_{j=1}^J \#A_j ln(\alpha_j) + \sum_{j=1}^J\sum_{i\in A_j}ln(\gamma_{\mu_j, v_j}(X_i))$ \end{center}
\underline{où} les $A_j$ sont définis par $A_j := \{ i\in \llbracket1,n \rrbracket$ tels que $Z_i = j \}$ \underline{i.e.} $\displaystyle\bigcup_{j=1}^J A_j = \llbracket1,n \rrbracket$
\end{prop}

\begin{proof}
La Log-vraisemblance du modèle s'écrit:
\begin{align*}
\mathcal{L}_\theta(X_1, \cdots, X_n, Z_1, \cdots, Z_n) &= ln\left(\displaystyle\prod_{i=1}^n h_\theta(X_i, Z_i) \right)\\
&= ln\left(\displaystyle\prod_{i=1}^n  \alpha_{Z_i}\gamma_{\mu_j, v_j}(X_i) \right)\\
&= \displaystyle\sum_{i=1}^n ln(\alpha_{Z_i})+ ln(\gamma_{\mu_j, v_j}(X_i))\\
\end{align*}
$Z_i$ est à valeur dans $\llbracket1,J \rrbracket$, on  partitionne donc $I := \llbracket1,n \rrbracket$ comme $I = \displaystyle\bigcup_{j=1}^J A_j$ pour obtenir
\begin{align*}
\mathcal{L}_\theta(X_1, \cdots, X_n, Z_1, \cdots, Z_n) &=  \displaystyle\sum_{j=1}^J\sum_{i\in A_j} ln(\alpha_{Z_i})+ \sum_{j=1}^J\sum_{i\in A_j} ln(\gamma_{\mu_j, v_j}(X_i))\\
&= \displaystyle\sum_{j=1}^J\sum_{i\in A_j} ln(\alpha_j)+ \sum_{j=1}^J\sum_{i\in A_j} ln(\gamma_{\mu_j, v_j}(X_i)) \\
&= \displaystyle\sum_{j=1}^J \#A_j ln(\alpha_j)+ \sum_{j=1}^J\sum_{i\in A_j} ln(\gamma_{\mu_j, v_j}(X_i)) \\
\end{align*}
\end{proof}
Nous pouvons dès lors maximiser la log-vraisemblance afin d'obtenir les estimateurs souhaités :
\begin{prop}[Estimateurs]
Les estimateurs du maximum de vraisemblance $\hat{\alpha_j}$ (resp. $\hat{\mu_j}$, et $\hat{v_j}$) de $\alpha_j$ (resp. $\mu_j$ et $v_j$) sont donnés par
\begin{center}
$\hat{\alpha_j} = \frac{\#A_j}{n}$
\end{center}
\begin{center}
$ \hat{\mu_j} = \displaystyle\frac{\sum_{i\in A_j} X_i}{\#A_j} $
\end{center}
\begin{center}
$ \hat{v_j} = \displaystyle \frac{\sum_{i\in A_j}(X_i - \hat{\mu_j})^2}{\#A_j}$
\end{center}
\end{prop}

\begin{proof}
Soit $\theta = (\alpha_j, \mu_j, v_j)_{j \in \llbracket 1,J \rrbracket}$. Il s'agit de déterminer 
\begin{center}
	$\underset{\theta\in\R^{3J}, \sum_{j=1}^J\alpha_j = 1}{\text{argmax}}\left(\displaystyle\sum_{j=1}^J \#A_j ln(\alpha_j)+ \sum_{j=1}^J\sum_{i\in A_j} ln(\gamma_{\mu_j, v_j}(x_i))\right)$
\end{center}
Nous avons donc à résoudre un programme de minimisation d'une fonction convexe sur un convexe avec une contraire égalité, il est ainsi naturel de faire appel au Lagrangien. \newline 
Ce dernier s'écrit
\begin{align*} 
L(\theta) &= \displaystyle\sum_{j=1}^J \#A_j ln(\alpha_j)+ \sum_{j=1}^J\sum_{i\in A_j} ln(\gamma_{\mu_j, v_j}(x_i)) - \lambda\times\left(\sum_{j=1}^J\alpha_j - 1\right)\\
&= \displaystyle\sum_{j=1}^J \#A_j ln(\alpha_j)+ \sum_{j=1}^J\sum_{i\in A_j} ln\left(\frac{1}{\sqrt{2\pi v_j}}\exp\left(-\frac{\left(x_i -\mu_j\right)^2}{2v_j} \right)\right) - \lambda\times\left(\sum_{j=1}^J\alpha_j - 1\right)\\
&= \displaystyle\sum_{j=1}^J \#A_j ln(\alpha_j)+ \sum_{j=1}^J\sum_{i\in A_j}\left( \frac{-1}{2}ln(2\pi v_j) -\frac{(x_i-\mu_j)^2}{2v_j}\right) - \lambda\times\left(\sum_{j=1}^J\alpha_j - 1\right)\\
\end{align*}

Il reste maintenant à résoudre le système suivant, afin d'obtenir le vecteur $\hat{\theta} := (\hat{\alpha_j}, \hat{\mu_j}, \hat{v_j})_{j\in\llbracket 1,J \rrbracket}$ solution du programme.

$
\begin{cases}
\displaystyle\frac{\#A_j}{\hat{\alpha_j}} - \lambda &= 0 \text{ } \forall j \in \llbracket 1,J \rrbracket \\
\displaystyle\sum_{i\in A_j} (x_i-\hat{\mu_j})/\hat{v_j} & = 0 \text{ } \forall j \in \llbracket 1,J \rrbracket \\
\displaystyle\sum_{i\in A_j} \frac{-0.5 \times 2 \times \pi}{2\pi \hat{v_j}} +\frac{(x_i-\hat{\mu_j})^2}{2\hat{v_j}^2} &= 0 \text{ } \forall j \in \llbracket 1,J \rrbracket\\
\displaystyle\sum_{j=1}^J \hat{\alpha_j} = 1
\end{cases}
$

Ceci équivaut à 

$
\begin{cases}
\displaystyle\frac{\#A_j}{\hat{\alpha_j}} &= \lambda \text{ } \forall j \in \llbracket 1,J \rrbracket \\
\displaystyle\sum_{i\in A_j}x_i & =\displaystyle\sum_{i\in A_j} \hat{\mu_j} \text{ } \forall j \in \llbracket 1,J \rrbracket \\
\displaystyle\sum_{i\in A_j} (x_i-\hat{\mu_j})^2 &= \displaystyle\sum_{i\in A_j} \hat{v_j}  \text{ } \forall j \in \llbracket 1,J \rrbracket \\
\displaystyle\sum_{j=1}^J \hat{\alpha_j} = 1
\end{cases}
$
$\Leftrightarrow$
$
\begin{cases}
\displaystyle\frac{\#A_j}{\hat{\alpha_j}} &= \lambda \text{ } \forall j \in \llbracket 1,J \rrbracket \\
\displaystyle\sum_{i\in A_j}\frac{x_i}{\#A_j} & = \hat{\mu_j} \text{ } \forall j \in \llbracket 1,J \rrbracket \\
\displaystyle\sum_{i\in A_j} \frac{(x_i-\hat{\mu_j})^2}{\#A_j} &=  \hat{v_j}  \text{ } \forall j \in \llbracket 1,J \rrbracket \\
\displaystyle\sum_{j=1}^J \hat{\alpha_j} = 1
\end{cases}
$

En sommant les $J$ premières lignes du système, on obtient $\displaystyle\sum_{j=1}^J\#A_j = \sum_{j=1}^J\hat{\alpha_j}\lambda$, \underline{i.e.} $\lambda = n$. En injectant ceci dans le précédent système, on obtient finalement ce qui était annoncé :

$
\begin{cases}
\hat{\alpha_j} &= \displaystyle \frac{\#A_j}{n} \text{ } \forall j \in \llbracket 1,J \rrbracket \\
\hat{\mu_j} &= \displaystyle\sum_{i\in A_j}\frac{x_i}{\#A_j} \text{ } \forall j \in \llbracket 1,J \rrbracket \\
\hat{v_j} &= \displaystyle\sum_{i\in A_j} \frac{(x_i-\hat{\mu_j})^2}{\#A_j}  \text{ } \forall j \in \llbracket 1,J \rrbracket \\
\end{cases}
$
\end{proof}


%section 3
\section{Le cas réel}
Nous nous placerons désormais dans un contexte tout autre que celui du paragraphe précédent, un contexte correspondant davantage à la réalité. Dans ce qui suit, nous supperons que ne sont observées que les tailles des nids, les diverses espèces de mouettes les ayant construit étant en quelques sortes des données "cachées". Nous avons donc un échantillon $X_1,\cdots, X_n$ de même loi que la variable $X$ comme définie ci-dessus. \newline
On définit $\mathcal{L}_{obs}$ la log-vraisemblance des observations, nous obtenons ainsi 

\begin{defn} 
La log-vraisemblance des observations s'écrit 
\begin{center} $\mathcal{L}_{obs}(\theta, X_1, \cdots, X_n) := ln\left( \displaystyle\prod_{i=1}^n f_\theta(X_i) \right) = \displaystyle\sum_{i=1}^nln\left( \sum_{j=1}^J \alpha_j \gamma_{\mu_j, v_j}(X_i) \right)$ \end{center}
\end{defn}

Nous voyons dès lors que l'existence d'une expression analytique du maximum de la log-vraisemblance n'est pas assurée. Il est donc nécessaire de trouver un moyen d'approcher les valeurs des différents estimateurs. \newline
Pour ce faire, on définit une log-vraisemblance des couples $(X_i,Z_i)$ sachant le vecteurs des observations $X_1, \cdots, X_n$.
\begin{prop}[log-vraisemblance conditionnelle]
On définit la log-vraisemblance $\lv_{c}(\theta, \thetat, X_1, \cdots, X_n) $ conditionnelle par
\begin{center} $\lv_{c}(\theta, \thetat, X_1, \cdots, X_n) = \E_{\thetat}[\lv_\theta(X_1, \cdots, X_n, Z_1, \cdots, Z_n) | X_1, \cdots, X_n]$ \end{center}
\end{prop}

\begin{prop}
On a
\begin{center} $\lv_{c}(\theta, \thetat, X_1, \cdots, X_n) = \displaystyle\sum_{i=1}^n\sum_{j=1}^J ln(h_\theta(X_i ,j))  \prob_{\thetat}(Z = j|X = X_i)$ \end{center}
\end{prop}

\begin{proof}
En effet
\begin{align*}
\lv_{c}(\theta, \thetat, X_1, \cdots, X_n) &= \E_{\thetat}[\lv_\theta(X_1, \cdots, X_n, Z_1, \cdots, X_n)|X_1, \cdots, X_n]\\
&=  \E_{\thetat}\bigg[\displaystyle ln\left(\prod_{i=1}^n  h_\theta(X_i,Z_i)\right) |X_1, \cdots X_n \bigg]\\
&= \displaystyle\sum_{i=1}^n  \E_{\thetat}[ln(h_\theta(X_i,Z_i))|X_1, \cdots, X_n]\\
\end{align*}
Or, les couples $(X_i,Z_i)$ sont indépendants, donc
\begin{align*}
\lv_{c}(\theta, \thetat, X_1, \cdots, X_n) &= \displaystyle\sum_{i=1}^n  \E_{\thetat}[ln(h_\theta(X_i,Z_i))|X_i ]\\
&= \displaystyle\sum_{i=1}^n \sum_{j = 1}^J ln(h_\theta(X_i,j)) \prob_{\thetat} (Z = j|X = X_i)\\
\end{align*}

\end{proof}

\begin{prop}[Expression log-vraisemblance conditionnelle] La fonction $\lv_{c}$ se réecrit sous la forme suivante : 
\begin{align*}
 \lv_{c}(\theta, \thetat, \bar{X}) &= -\frac{n}{2}log(2\pi)+\sum_{j=1}^J \left(\sum_{i = 1}^n g_{\thetat} (j|X=X_i)\right) log(\alpha(j))\\
&~~~-\frac{1}{2}\sum_{j=1}^J\left(\sum_{i=1}^n g_{\thetat}(j|X=X_i)\right)\left(log(v(j))+\frac{(X_i-m(j))^2}{v(j)}\right)
\end{align*}
\end{prop}

\begin{proof}
Il suffit de partir de la forme précédente de la log-vraisemblance conditionnelle, on a ainsi : 
\begin{align*}
 \lv_{c}(\theta, \thetat, \bar{X}) &= \sum_{i=1}^n \sum_{j = 1}^J ln(h_\theta(X_i,j)) \prob_{\thetat} (Z = j|X = X_i)\\
&= \sum_{i=1}^n \sum_{j = 1}^J ln(\alpha(j)\gamma_{m(j),v(j)}(X_i))\times \prob_{\thetat} (Z = j|X = X_i)\\
&= \sum_{i=1}^n \sum_{j = 1}^J \left(ln(\alpha(j))+ln(\gamma_{m(j),v(j)}(X_i))\right)\times \prob_{\thetat} (Z = j|X = X_i)\\
&= \sum_{i=1}^n \sum_{j = 1}^J ln(\alpha(j))\prob_{\thetat} (Z = j|X = X_i)+\sum_{i=1}^n \sum_{j = 1}^J ln(\gamma_{m(j),v(j)}(X_i))\prob_{\thetat} (Z = j|X = X_i)\\
\end{align*}
Traitons la double somme 
$\sum_{i=1}^n \sum_{j = 1}^J ln(\gamma_{m(j),v(j)}(X_i))\prob_{\thetat} (Z = j|X = X_i)$
on a :
\begin{center}
$\gamma_{m(j),v(j)}(X_i) = \frac{1}{\sqrt{2 \pi v(j)}}e^{-\frac{1}{2}\frac{(X_i - m(j))^2}{v(j)}}$
\end{center}
\begin{center}
$\prob_{\thetat} (Z = j|X = X_i) = \frac{\alpha(j)\gamma_{m(j),v(j)}}{\sum_{i=1}^J \alpha(j)\gamma_{m(i),v(i)}}$
\end{center}
La double somme devient alors 
\begin{center}
$\sum_{i=1}^n \sum_{j = 1}^J ln\left(\frac{1}{\sqrt{2 \pi v(j)}}e^{-\frac{1}{2}\frac{(X_i - m(j))^2}{v(j)}}\right)\prob_{\thetat} (Z = j|X = X_i)$
\end{center}
\begin{center}
$=\sum_{i=1}^n \sum_{j = 1}^J ln\left(\frac{1}{\sqrt{2 \pi v(j)}}\right)\prob_{\thetat} (Z = j|X = X_i) -\frac{1}{2}\left(\frac{(X_i - m(j))^2}{v(j)}\right)\prob_{\thetat} (Z = j|X = X_i) $
\end{center}
\begin{center}
$=\sum_{i=1}^n \sum_{j = 1}^J -\frac{1}{2} ln(2\pi) \prob_{\thetat} (Z = j|X = X_i)-\frac{1}{2}ln(v(j)) \prob_{\thetat} (Z = j|X = X_i) -\frac{1}{2}\left(\frac{(X_i - m(j))^2}{v(j)}\right)\prob_{\thetat} (Z = j|X = X_i) $
\end{center}
\begin{center}
$=-\frac{n}{2}ln(2\pi)  -\frac{1}{2} \sum_{i=1}^n \sum_{j = 1}^J \left(ln(v(j)) + \frac{(X_i - m(j))^2}{v(j)}\right)\prob_{\thetat} (Z = j|X = X_i) $
\end{center}
\begin{center}
$=-\frac{n}{2}ln(2\pi)  -\frac{1}{2} \sum_{j=1}^J \left( \sum_{i = 1}^n \prob_{\thetat} (Z = j|X = X_i) \right) \left(ln(v(j)) + \frac{(X_i - m(j))^2}{v(j)}\right) $
\end{center}
On obtient bien le résultat espéré :
\begin{align*}
 \lv_{c}(\theta, \thetat, \bar{X}) &= -\frac{n}{2}log(2\pi)+\sum_{j=1}^J \left(\sum_{i = 1}^n g_{\thetat} (j|X=X_i)\right) log(\alpha(j))\\
&~~~-\frac{1}{2}\sum_{j=1}^J\left(\sum_{i=1}^n g_{\thetat}(j|X=X_i)\right)\left(log(v(j))+\frac{(X_i-m(j))^2}{v(j)}\right)
\end{align*}
\end{proof}

\begin{prop}[Estimateurs du maximum de la log-vraisemblance conditionnelle] La fonction $\theta \mapsto \lv_{c}(\theta,\thetat,\bar{X})$ admet un unique maximum $\theta_M$ donné par : 
\begin{center}
$\alpha_M(j) = \frac{1}{n}\sum_{i=1}^n g_{\thetat}(j|X=X_i)$
\end{center}
\begin{center}

$m_M(j) = \frac{\sum_{i=1}^n X_i g_{\thetat}(j|X=X_i)}{\sum{i=1}^n g_{\thetat}(j|X=X_i)}$
\end{center}
\begin{center}

$v_M(j) = \frac{\sum_{i=1}^n (X_i -m(j))^2 g_{\thetat}(j|X=X_i)}{\sum{i=1}^n g_{\thetat}(j|X=X_i)}$
\end{center}

\end{prop}

\begin{proof}
hop
\end{proof}




\pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% chapitre 2

\chapter{L'algorithme EM}


% section 1
\section{Quelques preuves}

\begin{thm}
Soit $(\theta_k)_{k\in \N}$ la suite de paramètres construite à l'aide de l'algorithme EM. La log-vraisemblance $\lv_{obs}$ des observations vérifie 
\begin{center} $\lv_{obs}(\theta_{k+1}, X_1, \cdots, X_n) \geq \lv_{obs}(\theta_k, X_1, \cdots, X_n)$ \end{center}
\end{thm}

\begin{proof}

Nous allons commencer cette preuve en donnant une autre forme de la log-vraisemblance, dépendant de $\lv_ {obs}(\theta, X_1, \cdots, X_n)$ et d'un terme $\kappa_{\theta,\theta_k}$. Nous avons :
\begin{align*}
\lv_c(\theta, \theta_k, X_1, \cdots, X_n) &=  \displaystyle\sum_{i=1}^n\sum_{j=1}^J ln(h_\theta(X_i ,j))  \prob_{\theta_k}(Z = j|X = X_i)\\
&=  \displaystyle\sum_{i=1}^n\sum_{j=1}^J ln\big[f_\theta(X_i)\times \prob_\theta(Z = j|X=X_i)\big]\prob_{\theta_k}(Z = j|X = X_i)\\
&=  \displaystyle\sum_{i=1}^n\sum_{j=1}^J ln(f_\theta(X_i))\prob_{\theta_k}(Z = j|X = X_i) + \sum_{i=1}^n\sum_{j=1}^J ln(\prob_\theta(Z = j|X=X_i))\prob_{\theta_k}(Z = j|X = X_i)\\
&= \displaystyle\sum_{i=1}^n ln(f_\theta(X_i))\times\underbrace{\sum_{j=1}^J \prob_{\theta_k}(Z = j|X = X_i)}_{=1} + \sum_{i=1}^n\sum_{j=1}^J ln(\prob_\theta(Z = j|X = X_i))\prob_{\theta_k}(Z = j|X = X_i)\\
&= \displaystyle\sum_{i=1}^n ln(f_\theta(X_i)) + \sum_{i=1}^n\sum_{j=1}^J ln(\prob_\theta(Z = j|X=X_i))\prob_{\theta_k}(Z = j|X = X_i) \\
&= \lv_{obs}(\theta, X_1, \cdots, X_n) + \kappa_{\theta,\theta_k}
\end{align*}

Dès lors, on obtient
\begin{align*}
\lv_{obs}(\theta_{k+1}, X_1, \cdots, X_n) - \lv_{obs}(\theta_k, X_1, \cdots, X_n) &= \lv_c(\theta_{k+1}, \theta_k, X_1, \cdots, X_n) - \kappa_{\theta_{k+1}, \theta_k} -  \lv_c(\theta_k, \theta_k, X_1, \cdots, X_n) + \kappa_{\theta_{k}, \theta_k}
\end{align*}

Or, la quantité $\lv_c$ est maximisée en $\theta_{k+1}$ lors de l'étape $M$ de l'algorithme, donc
\begin{center} $ \lv_c(\theta_{k+1}, \theta_k, X_1, \cdots, X_n) - \lv_c(\theta_k, \theta_k, X_1, \cdots, X_n) \geq 0$ \end{center}

Il reste donc à prouver que 
\begin{center} $\kappa_{\theta_{k}, \theta_k}-\kappa_{\theta_{k+1}, \theta_k} \geq 0$ \end{center}

En effet, nous avons
\begin{align*}
\kappa_{\theta_{k}, \theta_k}-\kappa_{\theta_{k+1}, \theta_k} &= \sum_{i=1}^n\sum_{j=1}^J ln(\prob_{\theta_k}(Z = j|X=X_i))\prob_{\theta_k}(Z = j|X = X_i) \\
&- \sum_{i=1}^n \sum_{j=1}^J ln(\prob_{\theta_{k+1}}(Z = j|X=X_i))\prob_{\theta_k}(Z = j|X = X_i) \\
&= \sum_{i=1}^n \sum_{j=1}^J ln\left(\frac{\prob_{\theta_k}(Z = j|X=X_i)}{\prob_{\theta_{k+1}}(Z = j|X=X_i)}\right)\prob_{\theta_k}(Z = j|X = X_i)\\
&= - \sum_{i=1}^n \sum_{j=1}^J ln\left(\frac{\prob_{\theta_{k+1}}(Z = j|X=X_i)}{\prob_{\theta_k}(Z = j|X=X_i)}\right)\prob_{\theta_k}(Z = j|X = X_i)\\
&\geq - \sum_{i=1}^n ln\left(\sum_{j=1}^J \frac{\prob_{\theta_{k+1}}(Z = j|X=X_i)}{\prob_{\theta_k}(Z = j|X=X_i)}\prob_{\theta_k}(Z = j|X = X_i) \right)\\
&\text{$\big[$Cette dernière inégalité est due à la convexité du $log$ et au fait que $\displaystyle\sum_{j=1}^J\prob_{\theta_k}(Z = j|X)=1\big]$}\\
&= - \sum_{i=1}^n ln\left(\sum_{j=1}^J\prob_{\theta_{k+1}}(Z = j|X=X_i)\right)\\
&=  - \sum_{i=1}^n ln(1)\\
&= 0
\end{align*}


On obtient ainsi

\begin{center} $\kappa_{\theta_{k}, \theta_k}-\kappa_{\theta_{k+1}, \theta_k} \geq 0$ \end{center}


Et finalement
\begin{center} $\lv_{obs}(\theta_{k+1}, X_1, \cdots, X_n) \geq \lv_{obs}(\theta_k, X_1, \cdots, X_n)$ \end{center}

\end{proof}


\begin{thm}[Non monotonie de la vraisemblance]

\end{thm}

% section 2
\section{Pseudo code de l'algorithme EM}
Pour l'implémentation de cet algorithme, nous nous sommes appuyés sur le pseudo-code suivant.

\begin{algorithm}
	\caption{\textbf{L’algorithme EM (Dempster et al., 1977).}}
	\begin{algorithmic}[1]
		\REQUIRE{$N \in \mathbb{N}$, $\widehat{\theta_0} \in \Theta$, un jeu de données $y_1 \dots y_n$;}
		\ENSURE
		\STATE {$k:=1$;}
		\WHILE {$K < N + 1$}
		\STATE {$\text{\textbf{ETAPE E :} \textit{Calculer la fonction }} Q(\theta;\widehat{\theta}_{k-1}) = \frac{1}{n} \sum_{i=1}^{n} \mathbb{E}_{\widehat{\theta}_{k-1}} [log f(X_i,Y_i,\theta)|Y_i = y_i]$;}
		\STATE {$\text{\textbf{ETAPE M : }} \widehat{\theta}_k = argmax \hspace{1.5mm} Q(\theta;\widehat{\theta}_{k-1})$;}
		\STATE {$k \leftarrow k+1$;}
		\ENDWHILE
		\RETURN {$\widehat{\theta}_N$;}
	\end{algorithmic}
\end{algorithm}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter*{Bibliographie}
\addcontentsline{toc}{part}{Bibliographie}
 
Liens utiles

\url{https://www.lpsm.paris/pageperso/rebafka/BookGraphes/algorithme-em.html} \newline
\url{https://members.loria.fr/moberger/Enseignement/AVR/Exposes/algo-em.pdf} \newline
\url{http://faculty.washington.edu/fxia/courses/LING572/EM_collins97.pdf}\newline
\url{https://core.ac.uk/download/pdf/155777956.pdf}\newline
\url{http://www.cmap.polytechnique.fr/~bansaye/CoursTD6.pdf}


\pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{appendix}
\chapter{Annexe}

\end{appendix}

\end{document}
